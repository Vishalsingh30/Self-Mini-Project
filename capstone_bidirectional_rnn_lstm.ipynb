{"cells":[{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":23275,"status":"ok","timestamp":1734186403360,"user":{"displayName":"Vinay Moharir","userId":"16254761846626058829"},"user_tz":-330},"id":"s7-oG3VwUP5t","outputId":"a7daef85-be7c-46f1-fb12-0beb03ca3d23"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":399,"status":"ok","timestamp":1734186406773,"user":{"displayName":"Vinay Moharir","userId":"16254761846626058829"},"user_tz":-330},"id":"yr3Nn3PAUQgo"},"outputs":[],"source":["Path = '/content/drive/MyDrive/Colab Notebooks/Capstone_VinayM/data/'"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":7391,"status":"ok","timestamp":1734186415477,"user":{"displayName":"Vinay Moharir","userId":"16254761846626058829"},"user_tz":-330},"id":"ZE-LKmj3vBtj"},"outputs":[],"source":["import pandas as pd\n","import re\n","import tensorflow as tf\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","import numpy as np\n","import os\n","from nltk.stem import PorterStemmer"]},{"cell_type":"markdown","metadata":{"id":"wB7KUKfSaxSz"},"source":["**Step 1: Import and merge all the three datasets**"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":398,"status":"ok","timestamp":1734186421102,"user":{"displayName":"Vinay Moharir","userId":"16254761846626058829"},"user_tz":-330},"id":"3bYX41cbUgpH"},"outputs":[],"source":["\n","# Load each English and German file\n","german_files = [\"commoncrawl_de_en.txt\", \"europarl-v7_de_en.txt\", \"news-commentary-v9_de_en.txt\"]\n","english_files = [\"commoncrawl_en_de.txt\", \"europarl-v7_en_de.txt\", \"news-commentary-v9_en_de.txt\"]"]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":15582,"status":"ok","timestamp":1734186437985,"user":{"displayName":"Vinay Moharir","userId":"16254761846626058829"},"user_tz":-330},"id":"GNaVM1_-YVCC"},"outputs":[],"source":["\n","# Read English and German text files\n","with open(Path + 'commoncrawl_en_de.txt', 'r', encoding='utf-8') as eng_file:\n","    english_sentences = eng_file.readlines()\n","\n","with open(Path + 'commoncrawl_de_en.txt', 'r', encoding='utf-8') as ger_file:\n","    german_sentences = ger_file.readlines()\n","\n","# Strip any unnecessary whitespace (e.g., newline characters)\n","commoncrawl_english_sentences = [sentence.strip() for sentence in english_sentences]\n","commoncrawl_german_sentences = [sentence.strip() for sentence in german_sentences]"]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":387,"status":"ok","timestamp":1734186440408,"user":{"displayName":"Vinay Moharir","userId":"16254761846626058829"},"user_tz":-330},"id":"X3tRGwWGb87X"},"outputs":[],"source":["if len(commoncrawl_english_sentences) != len(commoncrawl_german_sentences):\n","    raise ValueError(\"The number of sentences in the English and German files do not match.\")"]},{"cell_type":"code","execution_count":8,"metadata":{"executionInfo":{"elapsed":1981,"status":"ok","timestamp":1734186446375,"user":{"displayName":"Vinay Moharir","userId":"16254761846626058829"},"user_tz":-330},"id":"saFKQuzfcCAq"},"outputs":[],"source":["commoncrawl_en_de_df = pd.DataFrame({\n","    'English': commoncrawl_english_sentences,\n","    'German': commoncrawl_german_sentences\n","})"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":665,"status":"ok","timestamp":1734186448520,"user":{"displayName":"Vinay Moharir","userId":"16254761846626058829"},"user_tz":-330},"id":"WBJcQmmicFKt","outputId":"504fad7a-2cb8-4168-90b1-34ff1d8e8ae5"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(2399123, 2)"]},"metadata":{},"execution_count":9}],"source":["commoncrawl_en_de_df.shape"]},{"cell_type":"code","execution_count":10,"metadata":{"executionInfo":{"elapsed":11418,"status":"ok","timestamp":1734186461969,"user":{"displayName":"Vinay Moharir","userId":"16254761846626058829"},"user_tz":-330},"id":"MOXbQ5bYcGj4"},"outputs":[],"source":["# Read English and German text files\n","with open(Path + 'europarl-v7_en_de.txt', 'r', encoding='utf-8') as eng_file:\n","    english_sentences = eng_file.readlines()\n","\n","with open(Path + 'europarl-v7_de_en.txt', 'r', encoding='utf-8') as ger_file:\n","    german_sentences = ger_file.readlines()\n","\n","# Strip any unnecessary whitespace (e.g., newline characters)\n","europarl_english_sentences = [sentence.strip() for sentence in english_sentences]\n","europarl_german_sentences = [sentence.strip() for sentence in german_sentences]"]},{"cell_type":"code","execution_count":11,"metadata":{"executionInfo":{"elapsed":381,"status":"ok","timestamp":1734186464503,"user":{"displayName":"Vinay Moharir","userId":"16254761846626058829"},"user_tz":-330},"id":"TzVxUEyhdZqI"},"outputs":[],"source":["if len(europarl_english_sentences) != len(europarl_german_sentences):\n","    raise ValueError(\"The number of sentences in the English and German files do not match.\")"]},{"cell_type":"code","execution_count":12,"metadata":{"executionInfo":{"elapsed":638,"status":"ok","timestamp":1734186466588,"user":{"displayName":"Vinay Moharir","userId":"16254761846626058829"},"user_tz":-330},"id":"7VIbwnMldiFw"},"outputs":[],"source":["europarl_en_de_df = pd.DataFrame({\n","    'English': europarl_english_sentences,\n","    'German': europarl_english_sentences\n","})"]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":394,"status":"ok","timestamp":1734186468330,"user":{"displayName":"Vinay Moharir","userId":"16254761846626058829"},"user_tz":-330},"id":"pP6xZnhsbbKl","outputId":"fd19a0cd-4fc8-48c8-86e0-a3a1bcc6259d"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(1920209, 2)"]},"metadata":{},"execution_count":13}],"source":["europarl_en_de_df.shape"]},{"cell_type":"code","execution_count":14,"metadata":{"executionInfo":{"elapsed":7141,"status":"ok","timestamp":1734186476946,"user":{"displayName":"Vinay Moharir","userId":"16254761846626058829"},"user_tz":-330},"id":"-_oDY4EddqnI"},"outputs":[],"source":["# Read English and German text files\n","# cleaning data for news comm in german\n","def is_special_characters(line):\n","    return re.match(r'^[^a-zA-Z0-9\\s]+$', line) is not None\n","\n","commentary_english_sentences = []\n","commentary_german_sentences = []\n","\n","with open(Path + 'news-commentary-v9_en_de.txt', 'r', encoding='utf-8') as file:\n","  for line in file:\n","        cleaned_line = line.strip()\n","\n","        if cleaned_line and not cleaned_line.isdigit() and not is_special_characters(cleaned_line):\n","            # Replace multiple spaces with a single space\n","            cleaned_line = re.sub(r'\\s+', ' ', cleaned_line)\n","            commentary_english_sentences.append(cleaned_line)\n","\n","with open(Path + 'news-commentary-v9_en_de.txt', 'r', encoding='utf-8') as file:\n","  for line in file:\n","        cleaned_line = line.strip()\n","\n","        if cleaned_line and not cleaned_line.isdigit() and not is_special_characters(cleaned_line):\n","            # Replace multiple spaces with a single space\n","            cleaned_line = re.sub(r'\\s+', ' ', cleaned_line)\n","            commentary_german_sentences.append(cleaned_line)"]},{"cell_type":"code","execution_count":15,"metadata":{"executionInfo":{"elapsed":399,"status":"ok","timestamp":1734186480838,"user":{"displayName":"Vinay Moharir","userId":"16254761846626058829"},"user_tz":-330},"id":"_Kgdo_5Pc9sd"},"outputs":[],"source":["if len(commentary_english_sentences) != len(commentary_german_sentences):\n","    raise ValueError(\"The number of sentences in the English and German files do not match.\")"]},{"cell_type":"code","execution_count":16,"metadata":{"executionInfo":{"elapsed":392,"status":"ok","timestamp":1734186482806,"user":{"displayName":"Vinay Moharir","userId":"16254761846626058829"},"user_tz":-330},"id":"JtNSJ7DddDxO"},"outputs":[],"source":["commentary_en_de_df = pd.DataFrame({\n","    'English': commentary_english_sentences,\n","    'German': commentary_german_sentences\n","})"]},{"cell_type":"code","execution_count":17,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1734186484126,"user":{"displayName":"Vinay Moharir","userId":"16254761846626058829"},"user_tz":-330},"id":"9t_PaaeldWdu","outputId":"c056e74c-8b63-4fe5-9359-575aa9048411"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(201553, 2)"]},"metadata":{},"execution_count":17}],"source":["commentary_en_de_df.shape"]},{"cell_type":"code","execution_count":18,"metadata":{"executionInfo":{"elapsed":404,"status":"ok","timestamp":1734186486238,"user":{"displayName":"Vinay Moharir","userId":"16254761846626058829"},"user_tz":-330},"id":"I-L0K10bdc-m"},"outputs":[],"source":["final_df = pd.concat([commoncrawl_en_de_df, europarl_en_de_df, commentary_en_de_df], axis=0, ignore_index=True)"]},{"cell_type":"code","execution_count":19,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":14,"status":"ok","timestamp":1734186487570,"user":{"displayName":"Vinay Moharir","userId":"16254761846626058829"},"user_tz":-330},"id":"lLQo-HlpdxIx","outputId":"2058d80e-652f-482b-99ef-bff425337955"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(4520885, 2)"]},"metadata":{},"execution_count":19}],"source":["final_df.shape"]},{"cell_type":"markdown","metadata":{"id":"eOcR8z2FbasV"},"source":["**Step 2: Data cleansing**"]},{"cell_type":"code","execution_count":20,"metadata":{"executionInfo":{"elapsed":372,"status":"ok","timestamp":1734186492710,"user":{"displayName":"Vinay Moharir","userId":"16254761846626058829"},"user_tz":-330},"id":"7AJy4TSkAj4O"},"outputs":[],"source":["train_df = final_df.sample(frac=0.006, random_state=12)\n","hold_out_df = final_df.sample(frac=0.01, random_state=22)"]},{"cell_type":"code","execution_count":21,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":405,"status":"ok","timestamp":1734186496566,"user":{"displayName":"Vinay Moharir","userId":"16254761846626058829"},"user_tz":-330},"id":"MsYQXaFEPc1Q","outputId":"5a5d4dad-4408-4368-c7d4-6d491f281274"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(4520885, 2)"]},"metadata":{},"execution_count":21}],"source":["final_df.shape"]},{"cell_type":"code","execution_count":22,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3069,"status":"ok","timestamp":1734186501313,"user":{"displayName":"Vinay Moharir","userId":"16254761846626058829"},"user_tz":-330},"id":"89j_R0lRfV4o","outputId":"62f8b7a0-0a7c-4a34-90ca-efd79aec5265"},"outputs":[{"output_type":"stream","name":"stdout","text":["                                                   English  \\\n","2067265           the counterpart appeared two years later   \n","152051             public parking is available metres away   \n","1684613  we guarantee the return of amount paid for the...   \n","2757674  that question was put to the citizens of switz...   \n","3663922  we welcome that because we the united states n...   \n","...                                                    ...   \n","63578    the staff was very nice and everything is goo ...   \n","2478570  however if we arrive in paris ten minutes late...   \n","46256    there's much to explore in this area and the m...   \n","3853028  we need to deal with increased globalisation a...   \n","2860401  mr president let me start by acknowledging the...   \n","\n","                                                    German  \n","2067265         das gegenstück erscheint zwei jahre später  \n","152051   öffentliche parkmöglichkeiten befinden sich in...  \n","1684613  wir garantieren die zurückzahlung der für die ...  \n","2757674  that question was put to the citizens of switz...  \n","3663922  we welcome that because we the united states n...  \n","...                                                    ...  \n","63578    the price was reasonable and provided very goo...  \n","2478570  however if we arrive in paris ten minutes late...  \n","46256    hier gibt es viel zu entdecken die vom damplat...  \n","3853028  we need to deal with increased globalisation a...  \n","2860401  mr president let me start by acknowledging the...  \n","\n","[27125 rows x 2 columns]\n"]}],"source":["def clean_text(text):\n","    # Convert text to lowercase\n","    text = text.lower()\n","    # Remove special characters and punctuation (except apostrophes in contractions)\n","    text = re.sub(r\"[^a-zA-ZäöüßÄÖÜéèàùâêîôûç'\\s]\", '', text)\n","    # Remove extra whitespace\n","    text = re.sub(r'\\s+', ' ', text).strip()\n","    return text\n","\n","# Apply cleaning function to both columns\n","train_df['English'] = train_df['English'].apply(clean_text)\n","train_df['German'] = train_df['German'].apply(clean_text)\n","\n","hold_out_df['English'] = hold_out_df['English'].apply(clean_text)\n","hold_out_df['German'] = hold_out_df['German'].apply(clean_text)\n","\n","# Display cleaned DataFrame\n","print(train_df)"]},{"cell_type":"markdown","metadata":{"id":"n2R8IGtFbjiV"},"source":["**Step 3: NLP pre processing - Dataset suitable to be used for AIML model learning**"]},{"cell_type":"code","execution_count":23,"metadata":{"executionInfo":{"elapsed":401,"status":"ok","timestamp":1734186503553,"user":{"displayName":"Vinay Moharir","userId":"16254761846626058829"},"user_tz":-330},"id":"KM35qozGm3HG"},"outputs":[],"source":["train_df = train_df.reset_index()\n","hold_out_df = hold_out_df.reset_index()"]},{"cell_type":"code","execution_count":24,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"executionInfo":{"elapsed":416,"status":"ok","timestamp":1734186505283,"user":{"displayName":"Vinay Moharir","userId":"16254761846626058829"},"user_tz":-330},"id":"JFZHpxCOAzXu","outputId":"015deea2-e222-4772-dbdd-01242eb29c1c"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["     index                                            English  \\\n","0  2067265           the counterpart appeared two years later   \n","1   152051            public parking is available metres away   \n","2  1684613  we guarantee the return of amount paid for the...   \n","3  2757674  that question was put to the citizens of switz...   \n","4  3663922  we welcome that because we the united states n...   \n","\n","                                              German  \n","0         das gegenstück erscheint zwei jahre später  \n","1  öffentliche parkmöglichkeiten befinden sich in...  \n","2  wir garantieren die zurückzahlung der für die ...  \n","3  that question was put to the citizens of switz...  \n","4  we welcome that because we the united states n...  "],"text/html":["\n","  <div id=\"df-5a7b7ab3-3e4e-4f26-9ff5-5ee2fd922687\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>index</th>\n","      <th>English</th>\n","      <th>German</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>2067265</td>\n","      <td>the counterpart appeared two years later</td>\n","      <td>das gegenstück erscheint zwei jahre später</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>152051</td>\n","      <td>public parking is available metres away</td>\n","      <td>öffentliche parkmöglichkeiten befinden sich in...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>1684613</td>\n","      <td>we guarantee the return of amount paid for the...</td>\n","      <td>wir garantieren die zurückzahlung der für die ...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>2757674</td>\n","      <td>that question was put to the citizens of switz...</td>\n","      <td>that question was put to the citizens of switz...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>3663922</td>\n","      <td>we welcome that because we the united states n...</td>\n","      <td>we welcome that because we the united states n...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-5a7b7ab3-3e4e-4f26-9ff5-5ee2fd922687')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-5a7b7ab3-3e4e-4f26-9ff5-5ee2fd922687 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-5a7b7ab3-3e4e-4f26-9ff5-5ee2fd922687');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-cc04ef4a-4277-4d81-ac4c-65d99ab5cf59\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-cc04ef4a-4277-4d81-ac4c-65d99ab5cf59')\"\n","            title=\"Suggest charts\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-cc04ef4a-4277-4d81-ac4c-65d99ab5cf59 button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","variable_name":"train_df","summary":"{\n  \"name\": \"train_df\",\n  \"rows\": 27125,\n  \"fields\": [\n    {\n      \"column\": \"index\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1309407,\n        \"min\": 76,\n        \"max\": 4520868,\n        \"num_unique_values\": 27125,\n        \"samples\": [\n          1573633,\n          4275545,\n          1957448\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"English\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 26893,\n        \"samples\": [\n          \"all discussions and questions and answers about specialized content management systems s in this section comes\",\n          \"the teachings and energy work of monika mueller have their focus on the enhancement of selfconfidence profound health and joyful life based on moksha liberation and conscious way of living\",\n          \"the hotel lindenufer is located in the famous historical old town of berlinspandau central but quiet right in front of the confluence of the rivers havel and spree\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"German\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 26903,\n        \"samples\": [\n          \"es mu\\u00df den myelom patienten erlaubt sein eine aktive rolle innerhalb der politischen entscheidungsprozesse einzunehmen die sie betreffen insbesondere wenn es um die bereitstellung von pflege und um den zugang zu neuen und innovativen behandlungsmethoden geht\",\n          \"irgendwann ergibt es sich dann dass die erbauer wieder eine freie welt w\\u00fcnschen und ihre gesch\\u00f6pfe deaktivieren wollen da erheben sich aber die androiden \\u00fcber ihre sch\\u00f6pfer zumal sie diese unprogrammierbaren wesen f\\u00fcr unvollkommen halten was schlie\\u00dflich zu deren untergang f\\u00fchrt\",\n          \"reprogenetics can be understood as an extension of parents' fundamental motivation and desire to protect their children and give them all possible advantages in life\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"}},"metadata":{},"execution_count":24}],"source":["train_df.head()"]},{"cell_type":"code","execution_count":25,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":13,"status":"ok","timestamp":1734186506914,"user":{"displayName":"Vinay Moharir","userId":"16254761846626058829"},"user_tz":-330},"id":"khECX4cx-C0_","outputId":"f8e7eb12-45b1-42ef-97b5-93c3ae2d19f4"},"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"]}],"source":["import nltk\n","nltk.download('punkt_tab')\n","\n","from nltk.tokenize import word_tokenize"]},{"cell_type":"code","execution_count":26,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":383,"status":"ok","timestamp":1734186509432,"user":{"displayName":"Vinay Moharir","userId":"16254761846626058829"},"user_tz":-330},"id":"LqB3iDNc-T8B","outputId":"53c8e971-3f89-4a71-f523-3e309651b2f3"},"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n"]}],"source":["nltk.download('stopwords')\n","\n","from nltk.corpus import stopwords\n","\n","german_stop_words = stopwords.words('german')\n","english_stop_words = stopwords.words('english')"]},{"cell_type":"code","execution_count":27,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":28247,"status":"ok","timestamp":1734186539052,"user":{"displayName":"Vinay Moharir","userId":"16254761846626058829"},"user_tz":-330},"id":"McCsEHNwM83Q","outputId":"70d3e282-297e-4e2a-ae1f-bbaa3a709785"},"outputs":[{"output_type":"stream","name":"stdout","text":["num_chunk-----> 0\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-27-be82efb5eff6>:18: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  chunk['german_tokens'] = chunk['German'].apply(word_tokenize)\n","<ipython-input-27-be82efb5eff6>:19: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  chunk['english_tokens'] = chunk['English'].apply(word_tokenize)\n","<ipython-input-27-be82efb5eff6>:20: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  chunk['german_tokens'] = chunk['german_tokens'].apply(lambda tokens: [token for token in tokens if token not in german_stop_words])\n","<ipython-input-27-be82efb5eff6>:21: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  chunk['english_tokens'] = chunk['english_tokens'].apply(lambda tokens: [token for token in tokens if token not in english_stop_words])\n","<ipython-input-27-be82efb5eff6>:23: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  chunk['german_tokens'] = chunk['german_tokens'].apply(lambda tokens: [stemmer.stem(token) for token in tokens])\n","<ipython-input-27-be82efb5eff6>:24: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  chunk['english_tokens'] = chunk['english_tokens'].apply(lambda tokens: [stemmer.stem(token) for token in tokens])\n","<ipython-input-27-be82efb5eff6>:25: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  chunk['German'] = chunk['german_tokens'].apply(lambda tokens: ' '.join(tokens))\n","<ipython-input-27-be82efb5eff6>:26: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  chunk['English'] = chunk['english_tokens'].apply(lambda tokens: ' '.join(tokens))\n"]},{"output_type":"stream","name":"stdout","text":["Data processing in chunks completed. Tokenized sequences are saved to disk.\n"]}],"source":["chunk_size = 500000  # Adjust this based on available memory\n","max_seq_length = 50  # Adjust based on data analysis\n","output_dir = '/content/data1117/preprocessed_chunks_training/'\n","os.makedirs(output_dir, exist_ok=True)\n","\n","# Initialize tokenizers\n","stemmer = PorterStemmer()\n","english_tokenizer = Tokenizer()\n","german_tokenizer = Tokenizer()\n","\n","# Split the DataFrame into chunks manually\n","num_chunks = len(train_df) // chunk_size + (1 if len(train_df) % chunk_size != 0 else 0)\n","\n","# Step 1: Fit Tokenizers Across Chunks\n","for i in range(num_chunks):\n","    print(\"num_chunk----->\", i)\n","    chunk = train_df.iloc[i * chunk_size:(i + 1) * chunk_size]\n","    chunk['german_tokens'] = chunk['German'].apply(word_tokenize)\n","    chunk['english_tokens'] = chunk['English'].apply(word_tokenize)\n","    chunk['german_tokens'] = chunk['german_tokens'].apply(lambda tokens: [token for token in tokens if token not in german_stop_words])\n","    chunk['english_tokens'] = chunk['english_tokens'].apply(lambda tokens: [token for token in tokens if token not in english_stop_words])\n","\n","    chunk['german_tokens'] = chunk['german_tokens'].apply(lambda tokens: [stemmer.stem(token) for token in tokens])\n","    chunk['english_tokens'] = chunk['english_tokens'].apply(lambda tokens: [stemmer.stem(token) for token in tokens])\n","    chunk['German'] = chunk['german_tokens'].apply(lambda tokens: ' '.join(tokens))\n","    chunk['English'] = chunk['english_tokens'].apply(lambda tokens: ' '.join(tokens))\n","\n","    # Update tokenizers\n","    english_tokenizer.fit_on_texts(chunk['English'])\n","    german_tokenizer.fit_on_texts(chunk['German'])\n","\n","    # Convert to sequences\n","    english_sequences = english_tokenizer.texts_to_sequences(chunk['English'])\n","    german_sequences = german_tokenizer.texts_to_sequences(chunk['German'])\n","\n","    # Pad sequences\n","    english_padded = pad_sequences(english_sequences, maxlen=max_seq_length, padding='post')\n","    german_padded = pad_sequences(german_sequences, maxlen=max_seq_length, padding='post')\n","\n","    # Save each chunk\n","    np.save(os.path.join(output_dir, f'english_chunk_{i}.npy'), english_padded)\n","    np.save(os.path.join(output_dir, f'german_chunk_{i}.npy'), german_padded)\n","\n","print(\"Data processing in chunks completed. Tokenized sequences are saved to disk.\")\n"]},{"cell_type":"code","execution_count":28,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":46079,"status":"ok","timestamp":1734186613221,"user":{"displayName":"Vinay Moharir","userId":"16254761846626058829"},"user_tz":-330},"id":"A8QJMlbbhDhd","outputId":"128d7e73-fa91-47ca-e659-893dbcbdd055"},"outputs":[{"output_type":"stream","name":"stdout","text":["num_chunk-----> 0\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-28-ff2f5c8d65ae>:18: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  chunk['german_tokens'] = chunk['German'].apply(word_tokenize)\n","<ipython-input-28-ff2f5c8d65ae>:19: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  chunk['english_tokens'] = chunk['English'].apply(word_tokenize)\n","<ipython-input-28-ff2f5c8d65ae>:20: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  chunk['german_tokens'] = chunk['german_tokens'].apply(lambda tokens: [token for token in tokens if token not in german_stop_words])\n","<ipython-input-28-ff2f5c8d65ae>:21: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  chunk['english_tokens'] = chunk['english_tokens'].apply(lambda tokens: [token for token in tokens if token not in english_stop_words])\n","<ipython-input-28-ff2f5c8d65ae>:23: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  chunk['german_tokens'] = chunk['german_tokens'].apply(lambda tokens: [stemmer.stem(token) for token in tokens])\n","<ipython-input-28-ff2f5c8d65ae>:24: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  chunk['english_tokens'] = chunk['english_tokens'].apply(lambda tokens: [stemmer.stem(token) for token in tokens])\n","<ipython-input-28-ff2f5c8d65ae>:25: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  chunk['German'] = chunk['german_tokens'].apply(lambda tokens: ' '.join(tokens))\n","<ipython-input-28-ff2f5c8d65ae>:26: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  chunk['English'] = chunk['english_tokens'].apply(lambda tokens: ' '.join(tokens))\n"]},{"output_type":"stream","name":"stdout","text":["Data processing in chunks completed. Tokenized sequences are saved to disk.\n"]}],"source":["chunk_size = 500000  # Adjust this based on available memory\n","max_seq_length = 50  # Adjust based on data analysis\n","output_dir = '/content/data1117/preprocessed_chunks_holdout/'\n","os.makedirs(output_dir, exist_ok=True)\n","\n","# Initialize tokenizers\n","stemmer = PorterStemmer()\n","english_tokenizer = Tokenizer()\n","german_tokenizer = Tokenizer()\n","\n","# Split the DataFrame into chunks manually\n","num_chunks = len(hold_out_df) // chunk_size + (1 if len(hold_out_df) % chunk_size != 0 else 0)\n","\n","# Step 1: Fit Tokenizers Across Chunks\n","for i in range(num_chunks):\n","    print(\"num_chunk----->\", i)\n","    chunk = hold_out_df.iloc[i * chunk_size:(i + 1) * chunk_size]\n","    chunk['german_tokens'] = chunk['German'].apply(word_tokenize)\n","    chunk['english_tokens'] = chunk['English'].apply(word_tokenize)\n","    chunk['german_tokens'] = chunk['german_tokens'].apply(lambda tokens: [token for token in tokens if token not in german_stop_words])\n","    chunk['english_tokens'] = chunk['english_tokens'].apply(lambda tokens: [token for token in tokens if token not in english_stop_words])\n","\n","    chunk['german_tokens'] = chunk['german_tokens'].apply(lambda tokens: [stemmer.stem(token) for token in tokens])\n","    chunk['english_tokens'] = chunk['english_tokens'].apply(lambda tokens: [stemmer.stem(token) for token in tokens])\n","    chunk['German'] = chunk['german_tokens'].apply(lambda tokens: ' '.join(tokens))\n","    chunk['English'] = chunk['english_tokens'].apply(lambda tokens: ' '.join(tokens))\n","\n","    # Update tokenizers\n","    english_tokenizer.fit_on_texts(chunk['English'])\n","    german_tokenizer.fit_on_texts(chunk['German'])\n","\n","    # Convert to sequences\n","    english_sequences = english_tokenizer.texts_to_sequences(chunk['English'])\n","    german_sequences = german_tokenizer.texts_to_sequences(chunk['German'])\n","\n","    # Pad sequences\n","    english_padded = pad_sequences(english_sequences, maxlen=max_seq_length, padding='post')\n","    german_padded = pad_sequences(german_sequences, maxlen=max_seq_length, padding='post')\n","\n","    # Save each chunk\n","    np.save(os.path.join(output_dir, f'english_chunk_{i}.npy'), english_padded)\n","    np.save(os.path.join(output_dir, f'german_chunk_{i}.npy'), german_padded)\n","\n","print(\"Data processing in chunks completed. Tokenized sequences are saved to disk.\")"]},{"cell_type":"code","execution_count":29,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":404,"status":"ok","timestamp":1734186777274,"user":{"displayName":"Vinay Moharir","userId":"16254761846626058829"},"user_tz":-330},"id":"2inGit1MX16j","outputId":"80806cc3-1697-4ae3-ee60-28583ed6588a"},"outputs":[{"output_type":"stream","name":"stdout","text":["Combined DataFrame created successfully.\n","                                             English  \\\n","0  [3371, 556, 58, 17, 706, 0, 0, 0, 0, 0, 0, 0, ...   \n","1  [86, 378, 120, 1378, 442, 0, 0, 0, 0, 0, 0, 0,...   \n","2  [449, 543, 492, 1293, 31, 0, 0, 0, 0, 0, 0, 0,...   \n","3  [87, 178, 210, 1663, 0, 0, 0, 0, 0, 0, 0, 0, 0...   \n","4  [264, 200, 8, 18, 820, 2483, 1394, 107, 21, 14...   \n","\n","                                              German  \n","0  [12720, 2338, 277, 131, 1308, 0, 0, 0, 0, 0, 0...  \n","1  [1501, 9944, 954, 305, 2263, 0, 0, 0, 0, 0, 0,...  \n","2  [3507, 12721, 226, 19194, 7197, 0, 0, 0, 0, 0,...  \n","3  [6, 132, 199, 3, 1, 182, 2, 6381, 0, 0, 0, 0, ...  \n","4  [10, 416, 6, 88, 10, 1, 290, 33, 53, 1086, 299...  \n"]}],"source":["# Initialize lists to store all data\n","english_data = []\n","german_data = []\n","output_dir = '/content/data1117/preprocessed_chunks_training/'\n","# Iterate through all saved chunks\n","num_chunks = len([name for name in os.listdir(output_dir) if name.startswith('english_chunk_')])\n","\n","for i in range(num_chunks):\n","    # Load the English and German chunks\n","    english_chunk = np.load(os.path.join(output_dir, f'english_chunk_{i}.npy'))\n","    german_chunk = np.load(os.path.join(output_dir, f'german_chunk_{i}.npy'))\n","\n","    # Append to the list\n","    english_data.extend(english_chunk)\n","    german_data.extend(german_chunk)\n","\n","# Convert lists to DataFrame\n","preprocessed_training_data = pd.DataFrame({\n","    'English': english_data,\n","    'German': german_data\n","})\n","\n","print(\"Combined DataFrame created successfully.\")\n","print(preprocessed_training_data.head())"]},{"cell_type":"code","execution_count":30,"metadata":{"executionInfo":{"elapsed":1195,"status":"ok","timestamp":1734186780757,"user":{"displayName":"Vinay Moharir","userId":"16254761846626058829"},"user_tz":-330},"id":"35gKhDymrfDT"},"outputs":[],"source":["preprocessed_training_data.to_parquet('/content/drive/MyDrive/Colab Notebooks/Capstone_VinayM/data/capstone_preprocess_training.parquet')"]},{"cell_type":"code","execution_count":31,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":420,"status":"ok","timestamp":1734186784056,"user":{"displayName":"Vinay Moharir","userId":"16254761846626058829"},"user_tz":-330},"id":"Kp-WpL0ehQUc","outputId":"e5ef183e-bebb-4646-f6c4-bd3aa08c85aa"},"outputs":[{"output_type":"stream","name":"stdout","text":["Combined DataFrame created successfully.\n","                                             English  \\\n","0  [10, 40, 199, 415, 42, 508, 0, 0, 0, 0, 0, 0, ...   \n","1  [111, 2, 2432, 2111, 2557, 798, 1051, 1512, 20...   \n","2  [3630, 11267, 1035, 16200, 3398, 839, 3631, 16...   \n","3  [547, 56, 57, 22, 1302, 1292, 9013, 2745, 1473...   \n","4  [347, 738, 766, 2279, 635, 428, 311, 1941, 140...   \n","\n","                                              German  \n","0  [1, 30, 23, 563, 3, 191, 1, 605, 2, 12, 88, 68...  \n","1  [5053, 7483, 6936, 9035, 10181, 1278, 0, 0, 0,...  \n","2  [664, 5054, 10182, 26879, 17785, 5659, 3905, 1...  \n","3  [13, 511, 6, 190, 139, 72, 1516, 48, 1615, 8, ...  \n","4  [1, 1429, 1, 2377, 4, 5660, 2, 1, 1343, 1, 422...  \n"]}],"source":["# Initialize lists to store all data\n","english_data = []\n","german_data = []\n","output_dir = '/content/data1117/preprocessed_chunks_holdout/'\n","# Iterate through all saved chunks\n","num_chunks = len([name for name in os.listdir(output_dir) if name.startswith('english_chunk_')])\n","\n","for i in range(num_chunks):\n","    # Load the English and German chunks\n","    english_chunk = np.load(os.path.join(output_dir, f'english_chunk_{i}.npy'))\n","    german_chunk = np.load(os.path.join(output_dir, f'german_chunk_{i}.npy'))\n","\n","    # Append to the list\n","    english_data.extend(english_chunk)\n","    german_data.extend(german_chunk)\n","\n","# Convert lists to DataFrame\n","preprocessed_holdout_data = pd.DataFrame({\n","    'English': english_data,\n","    'German': german_data\n","})\n","\n","print(\"Combined DataFrame created successfully.\")\n","print(preprocessed_holdout_data.head())"]},{"cell_type":"code","execution_count":32,"metadata":{"executionInfo":{"elapsed":1229,"status":"ok","timestamp":1734186787575,"user":{"displayName":"Vinay Moharir","userId":"16254761846626058829"},"user_tz":-330},"id":"-IUk8nVxhaM2"},"outputs":[],"source":["preprocessed_holdout_data.to_parquet('/content/drive/MyDrive/Colab Notebooks/Capstone_VinayM/data/capstone_preprocess_holdout.parquet')"]},{"cell_type":"code","execution_count":33,"metadata":{"executionInfo":{"elapsed":773,"status":"ok","timestamp":1734186793924,"user":{"displayName":"Vinay Moharir","userId":"16254761846626058829"},"user_tz":-330},"id":"hBwlZ0ZwzH-l"},"outputs":[],"source":["train_df = pd.read_parquet('/content/drive/MyDrive/Colab Notebooks/Capstone_VinayM/data/capstone_preprocess_training.parquet')\n","hold_out_df = pd.read_parquet('/content/drive/MyDrive/Colab Notebooks/Capstone_VinayM/data/capstone_preprocess_holdout.parquet')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"H5s3lLaR_LGs"},"outputs":[],"source":["# Saving the LSTM model\n","torch.save(model.state_dict(), \"/content/drive/MyDrive/Colab Notebooks/Capstone_VinayM/simple_lstm_model.pth\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jX6KV8A-_QxZ"},"outputs":[],"source":["loaded_model = SimpleLSTM(input_dim, output_dim, hidden_dim)\n","loaded_model.load_state_dict(torch.load(\"/content/drive/MyDrive/Colab Notebooks/Capstone_VinayM/simple_lstm_model.pth\"))\n","loaded_model.to(device)"]},{"cell_type":"markdown","source":["**Step 4: Design, train and test Bidirectional RNN**"],"metadata":{"id":"3Yf0uY5Rzw8s"}},{"cell_type":"code","execution_count":40,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2036502,"status":"ok","timestamp":1734191277235,"user":{"displayName":"Vinay Moharir","userId":"16254761846626058829"},"user_tz":-330},"id":"Nj5alWz3q9jJ","outputId":"e498bdfd-e603-40a6-a732-46c43f451f13"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/20, Loss: 3.101881661715935\n","Epoch 2/20, Loss: 2.6308111647530548\n","Epoch 3/20, Loss: 2.2397344134185673\n","Epoch 4/20, Loss: 1.942171069494677\n","Epoch 5/20, Loss: 1.7885547555491048\n","Epoch 6/20, Loss: 1.6670257261128358\n","Epoch 7/20, Loss: 1.58733650904922\n","Epoch 8/20, Loss: 1.5139870382845402\n","Epoch 9/20, Loss: 1.4414301098869093\n","Epoch 10/20, Loss: 1.4028230209676724\n","Epoch 11/20, Loss: 1.3475716442642909\n","Epoch 12/20, Loss: 1.3192388594923716\n","Epoch 13/20, Loss: 1.2920569619339592\n","Epoch 14/20, Loss: 1.2769018903831548\n","Epoch 15/20, Loss: 1.2354355149392813\n","Epoch 16/20, Loss: 1.2193441656154562\n","Epoch 17/20, Loss: 1.1917094620612432\n","Epoch 18/20, Loss: 1.2015229151672069\n","Epoch 19/20, Loss: 1.1768935860644252\n","Epoch 20/20, Loss: 1.1766289272609185\n"]}],"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import DataLoader, Dataset\n","\n","# Step 1: Define the Dataset Class\n","class SequenceDataset(Dataset):\n","    def __init__(self, df, input_col, target_col, input_max_len=None, target_max_len=None):\n","        self.input_sequences = self.pad_sequences(df[input_col].tolist(), max_length=input_max_len)\n","        self.target_sequences = self.pad_sequences(df[target_col].tolist(), max_length=target_max_len)\n","    def pad_sequences(self, sequences, max_length=None):\n","        valid_sequences = [seq for seq in sequences if len(seq) > 0]\n","        if max_length is None:\n","            max_length = max(len(seq) for seq in valid_sequences)\n","        return np.array(\n","            valid_sequences + [[0] * max_length for _ in range(len(sequences) - len(valid_sequences))],\n","            dtype=np.int32\n","        )\n","\n","    def __len__(self):\n","        return len(self.input_sequences)\n","\n","    def __getitem__(self, idx):\n","        return (\n","            torch.tensor(self.input_sequences[idx], dtype=torch.long),\n","            torch.tensor(self.target_sequences[idx], dtype=torch.long),\n","        )\n","\n","# Load and preprocess data\n","# Assume df is a Pandas DataFrame with \"English\" and \"German\" columns\n","input_max_len = 50  # Set maximum sequence lengths\n","target_max_len = 50\n","train_dataset = SequenceDataset(train_df, input_col=\"English\", target_col=\"German\", input_max_len=input_max_len, target_max_len=target_max_len)\n","hold_out_dataset = SequenceDataset(hold_out_df, input_col=\"English\", target_col=\"German\", input_max_len=input_max_len, target_max_len=target_max_len)\n","\n","# DataLoader\n","batch_size = 16\n","train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","holdout_dataloader = DataLoader(hold_out_dataset, batch_size=batch_size, shuffle=True)\n","\n","# Step 2: Define the Bidirectional RNN Model\n","class BidirectionalRNN(nn.Module):\n","    def __init__(self, input_dim, embed_dim, hidden_dim, output_dim):\n","        super(BidirectionalRNN, self).__init__()\n","        self.embedding = nn.Embedding(input_dim, embed_dim)\n","        self.rnn = nn.RNN(embed_dim, hidden_dim, batch_first=True, bidirectional=True)\n","        self.fc = nn.Linear(hidden_dim * 2, output_dim)  # Multiply by 2 for bidirectional\n","\n","    def forward(self, x):\n","        embedded = self.embedding(x)  # Convert token indices to embeddings\n","        output, _ = self.rnn(embedded)  #\n","        output = self.fc(output)  # Fully connected layer\n","        return output\n","\n","\n","\n","# Model parameters\n","input_dim = train_df[\"English\"].apply(lambda x: max(x)).max() + 1  # Vocabulary size of English\n","output_dim = train_df[\"German\"].apply(lambda x: max(x)).max() + 1  # Vocabulary size of German\n","embed_dim = 128  # Embedding dimension\n","hidden_dim = 256  # Hidden state dimension\n","batch_size = 32\n","num_epochs = 20\n","learning_rate = 0.001\n","\n","# Initialize model, loss, and optimizer\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model = BidirectionalRNN(input_dim, embed_dim, hidden_dim, output_dim).to(device)\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n","\n","# Training loop\n","for epoch in range(num_epochs):\n","    model.train()\n","    epoch_loss = 0\n","    for inputs, targets in train_dataloader:\n","        inputs, targets = inputs.to(device), targets.to(device)\n","\n","        # Forward pass\n","        outputs = model(inputs)\n","\n","        # Reshape outputs and targets for loss computation\n","        outputs = outputs.view(-1, output_dim)\n","        targets = targets.view(-1)\n","\n","        # Compute loss\n","        loss = criterion(outputs, targets)\n","\n","        # Backpropagation\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","        epoch_loss += loss.item()\n","\n","    print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {epoch_loss / len(train_dataloader)}\")\n","\n","# Evaluation function\n","# def evaluate_model(model, dataloader):\n","#     model.eval()\n","#     total_loss = 0\n","#     with torch.no_grad():\n","#         for inputs, targets in dataloader:\n","#             inputs, targets = inputs.to(device), targets.to(device)\n","\n","#             outputs = model(inputs)\n","#             outputs = outputs.view(-1, output_dim)\n","#             targets = targets.view(-1)\n","\n","#             loss = criterion(outputs, targets)\n","#             total_loss += loss.item()\n","#     return total_loss / len(dataloader)\n","\n","# # Test the model (using train data as test data for simplicity)\n","# test_loss = evaluate_model(model, train_dataloader)\n","# print(f\"Test Loss: {test_loss}\")"]},{"cell_type":"code","execution_count":41,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":999,"status":"ok","timestamp":1734191290920,"user":{"displayName":"Vinay Moharir","userId":"16254761846626058829"},"user_tz":-330},"id":"jCRd9FJEH2bR","outputId":"e55ab1d6-d5b8-40e2-cab5-54e148979b19"},"outputs":[{"output_type":"stream","name":"stdout","text":["Sample prediction: [  257 49608  7335  1673  3966 49609  1886   230     0     0     0     0\n","     0     0     0     0     0     0     0     0     0     0     0     0\n","     0     0     0     0     0     0     0     0     0     0     0     0\n","     0     0     0     0     0     0     0     0     0     0     0     0\n","     0     0]\n"]}],"source":["input_dim = train_df[\"English\"].apply(lambda x: max(x)).max() + 1  # Vocabulary size of English\n","model.eval()\n","with torch.no_grad():\n","    for inputs, targets in train_dataloader:\n","        inputs = inputs.to(device) # Move inputs to the correct device\n","        # Removed one-hot encoding - the embedding layer expects integer indices\n","        outputs = model(inputs)\n","        print(\"Sample prediction:\", torch.argmax(outputs, dim=2)[0].cpu().numpy())\n","        break"]},{"cell_type":"code","execution_count":42,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":34517,"status":"ok","timestamp":1734191328431,"user":{"displayName":"Vinay Moharir","userId":"16254761846626058829"},"user_tz":-330},"id":"ylA8mikcH7Ah","outputId":"a3e4d91b-eba7-43e0-8552-72cf674afd8b"},"outputs":[{"output_type":"stream","name":"stdout","text":["Average BLEU Score: 0.4147\n"]}],"source":["from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n","\n","# Define a function to compute BLEU score\n","def compute_bleu_score(reference, hypothesis):\n","    \"\"\"\n","    Compute BLEU score for a single pair of reference and hypothesis sequences.\n","    :param reference: List of integers (ground truth sequence)\n","    :param hypothesis: List of integers (predicted sequence)\n","    :return: BLEU score\n","    \"\"\"\n","    smoothing = SmoothingFunction().method1\n","    return sentence_bleu([reference], hypothesis, smoothing_function=smoothing)\n","\n","# Evaluate BLEU score on a batch\n","model.eval()\n","total_bleu = 0\n","count = 0\n","\n","with torch.no_grad():\n","    for inputs, targets in train_dataloader:\n","        # inputs_one_hot = torch.nn.functional.one_hot(inputs, num_classes=input_dim).float().to(device)\n","        inputs = inputs.to(device) # Move the inputs to the appropriate device\n","        targets = targets.to(device)\n","\n","        # Forward pass\n","        outputs = model(inputs) # Pass the original inputs (integer indices)\n","\n","        # Get the predicted sequences\n","        predictions = torch.argmax(outputs, dim=2)  # Shape: [batch_size, seq_length]\n","\n","        # Compute BLEU for each sequence in the batch\n","        for i in range(predictions.size(0)):\n","            reference = targets[i].cpu().tolist()\n","            hypothesis = predictions[i].cpu().tolist()\n","\n","            # Remove padding tokens (0s)\n","            reference = [token for token in reference if token != 0]\n","            hypothesis = [token for token in hypothesis if token != 0]\n","\n","            bleu_score = compute_bleu_score(reference, hypothesis)\n","            total_bleu += bleu_score\n","            count += 1\n","\n","# Compute the average BLEU score for the dataset\n","average_bleu = total_bleu / count\n","print(f\"Average BLEU Score: {average_bleu:.4f}\")"]},{"cell_type":"code","source":["# Saving the Bidirectional RNN model\n","torch.save(model.state_dict(), \"/content/drive/MyDrive/Colab Notebooks/Capstone_VinayM/Bidirectional_RNN_model.pth\")"],"metadata":{"id":"NcevGZ1Tz8-C","executionInfo":{"status":"ok","timestamp":1734191338325,"user_tz":-330,"elapsed":746,"user":{"displayName":"Vinay Moharir","userId":"16254761846626058829"}}},"execution_count":43,"outputs":[]},{"cell_type":"code","source":["loaded_model = BidirectionalRNN(input_dim, embed_dim, hidden_dim, output_dim)\n","loaded_model.load_state_dict(torch.load(\"/content/drive/MyDrive/Colab Notebooks/Capstone_VinayM/Bidirectional_RNN_model.pth\"))\n","loaded_model.to(device)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"M8dc61RS0MTz","executionInfo":{"status":"ok","timestamp":1734191344329,"user_tz":-330,"elapsed":1185,"user":{"displayName":"Vinay Moharir","userId":"16254761846626058829"}},"outputId":"dbcd0d52-fa39-4ac1-dd12-b3200925ccf4"},"execution_count":44,"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-44-df45d0496cb4>:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  loaded_model.load_state_dict(torch.load(\"/content/drive/MyDrive/Colab Notebooks/Capstone_VinayM/Bidirectional_RNN_model.pth\"))\n"]},{"output_type":"execute_result","data":{"text/plain":["BidirectionalRNN(\n","  (embedding): Embedding(30507, 128)\n","  (rnn): RNN(128, 256, batch_first=True, bidirectional=True)\n","  (fc): Linear(in_features=512, out_features=53853, bias=True)\n",")"]},"metadata":{},"execution_count":44}]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","\n","class Attention(nn.Module):\n","    def __init__(self, hidden_size):\n","        super(Attention, self).__init__()\n","        self.attn = nn.Linear(hidden_size * 2, hidden_size)\n","        self.v = nn.Parameter(torch.rand(hidden_size))\n","\n","    def forward(self, hidden, encoder_outputs):\n","        energy = torch.tanh(self.attn(encoder_outputs))  # (batch_size, seq_len, hidden_size)\n","        energy = energy.matmul(self.v)  # (batch_size, seq_len)\n","        attention_weights = torch.softmax(energy, dim=1)  # (batch_size, seq_len)\n","        return attention_weights\n","\n","class BiRNNWithAttention(nn.Module):\n","    def __init__(self, vocab_size, embedding_dim, hidden_size, output_size):\n","        super(BiRNNWithAttention, self).__init__()\n","        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n","        self.rnn = nn.RNN(embedding_dim, hidden_size, batch_first=True, bidirectional=True, num_layers=2, dropout=0.5)\n","        self.attention = Attention(hidden_size)\n","        self.fc = nn.Linear(hidden_size * 2, output_size)\n","\n","    def forward(self, x):\n","        x = self.embedding(x)\n","        rnn_out, _ = self.rnn(x)\n","        attention_weights = self.attention(rnn_out, rnn_out)\n","        context_vector = attention_weights.unsqueeze(1).bmm(rnn_out)  # weighted sum\n","        out = self.fc(context_vector.squeeze(1))  # Final prediction\n","        return out\n","\n","# Hyperparameters\n","vocab_size = 1000\n","embedding_dim = 50\n","hidden_size = 256\n","output_size = 1\n","batch_size = 32\n","sequence_length = 10\n","\n","# Model and optimizer\n","model = BiRNNWithAttention(vocab_size, embedding_dim, hidden_size, output_size)\n","optimizer = optim.Adam(model.parameters(), lr=0.001)\n","\n","# Training Loop (simplified)\n","for epoch in range(100):\n","    outputs = model(X_train)\n","    loss = criterion(outputs, y_train)\n","\n","    optimizer.zero_grad()\n","    loss.backward()\n","    optimizer.step()\n","\n","    print(f\"Epoch {epoch}, Loss: {loss.item():.4f}\")"],"metadata":{"id":"jL4drMySqiE6"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":37,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mikOsh6RLg-K","executionInfo":{"status":"ok","timestamp":1734189086360,"user_tz":-330,"elapsed":2060168,"user":{"displayName":"Vinay Moharir","userId":"16254761846626058829"}},"outputId":"1bf366e2-1cb7-4a23-c45d-0efc09ec4bd0"},"outputs":[{"output_type":"stream","name":"stdout","text":["cuda\n","Epoch 1/20, Loss: 2.8836493992580556\n","Epoch 2/20, Loss: 2.352052436614374\n","Epoch 3/20, Loss: 1.8609018999872342\n","Epoch 4/20, Loss: 1.4628527197846264\n","Epoch 5/20, Loss: 1.2348058274332083\n","Epoch 6/20, Loss: 1.093099350619288\n","Epoch 7/20, Loss: 0.9929564326193254\n","Epoch 8/20, Loss: 0.9148583354709564\n","Epoch 9/20, Loss: 0.8491544980590917\n","Epoch 10/20, Loss: 0.79494068612573\n","Epoch 11/20, Loss: 0.7493307876646659\n","Epoch 12/20, Loss: 0.7089088592977034\n","Epoch 13/20, Loss: 0.6756446088646662\n","Epoch 14/20, Loss: 0.645709201140013\n","Epoch 15/20, Loss: 0.620196433175566\n","Epoch 16/20, Loss: 0.5965943441216676\n","Epoch 17/20, Loss: 0.5755609249309549\n","Epoch 18/20, Loss: 0.555085060590843\n","Epoch 19/20, Loss: 0.5370818546419647\n","Epoch 20/20, Loss: 0.5233660059155159\n"]}],"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import DataLoader, Dataset\n","\n","\n","# Step 1: Define the Dataset Class\n","class SequenceDataset(Dataset):\n","    def __init__(self, df, input_col, target_col, input_max_len=None, target_max_len=None):\n","        self.input_sequences = self.pad_sequences(df[input_col].tolist(), max_length=input_max_len)\n","        self.target_sequences = self.pad_sequences(df[target_col].tolist(), max_length=target_max_len)\n","    def pad_sequences(self, sequences, max_length=None):\n","        valid_sequences = [seq for seq in sequences if len(seq) > 0]\n","        if max_length is None:\n","            max_length = max(len(seq) for seq in valid_sequences)\n","        return np.array(\n","            valid_sequences + [[0] * max_length for _ in range(len(sequences) - len(valid_sequences))],\n","            dtype=np.int32\n","        )\n","\n","    def __len__(self):\n","        return len(self.input_sequences)\n","\n","    def __getitem__(self, idx):\n","        return (\n","            torch.tensor(self.input_sequences[idx], dtype=torch.long),\n","            torch.tensor(self.target_sequences[idx], dtype=torch.long),\n","        )\n","\n","# Load and preprocess data\n","# Assume df is a Pandas DataFrame with \"English\" and \"German\" columns\n","input_max_len = 50  # Set maximum sequence lengths\n","target_max_len = 50\n","train_dataset = SequenceDataset(train_df, input_col=\"English\", target_col=\"German\", input_max_len=input_max_len, target_max_len=target_max_len)\n","hold_out_dataset = SequenceDataset(hold_out_df, input_col=\"English\", target_col=\"German\", input_max_len=input_max_len, target_max_len=target_max_len)\n","\n","# DataLoader\n","batch_size = 16\n","train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","holdout_dataloader = DataLoader(hold_out_dataset, batch_size=batch_size, shuffle=True)\n","\n","class BidirectionalLSTM(nn.Module):\n","    def __init__(self, input_dim, embed_dim, hidden_dim, output_dim):\n","        super(BidirectionalLSTM, self).__init__()\n","        self.embedding = nn.Embedding(input_dim, embed_dim)\n","        self.lstm = nn.LSTM(embed_dim, hidden_dim, batch_first=True, bidirectional=True)\n","        self.fc = nn.Linear(hidden_dim * 2, output_dim)  # Multiply by 2 for bidirectional\n","\n","    def forward(self, x):\n","        embedded = self.embedding(x)  # Convert token indices to embeddings\n","        output, _ = self.lstm(embedded)  # LSTM forward pass\n","        output = self.fc(output)  # Fully connected layer\n","        return output\n","\n","# Model parameters\n","input_dim = train_df[\"English\"].apply(lambda x: max(x)).max() + 1  # Vocabulary size of English\n","output_dim = train_df[\"German\"].apply(lambda x: max(x)).max() + 1  # Vocabulary size of German\n","embed_dim = 128\n","hidden_dim = 256\n","batch_size = 32\n","num_epochs = 20\n","learning_rate = 0.001\n","\n","# Initialize model, loss, and optimizer\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(device)\n","model = BidirectionalLSTM(input_dim, embed_dim, hidden_dim, output_dim).to(device)\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(model.parameters(), lr=0.001)\n","\n","# Step 3: Training Loop\n","#num_epochs = 10\n","for epoch in range(num_epochs):\n","    model.train()\n","    epoch_loss = 0\n","    for inputs, targets in train_dataloader:\n","        inputs = inputs.to(device)\n","        targets = targets.to(device)\n","\n","        # Forward pass\n","        outputs = model(inputs)\n","\n","        # Reshape outputs and targets for loss computation\n","        outputs = outputs.view(-1, output_dim)  # Flatten for CrossEntropyLoss\n","        targets = targets.view(-1)  # Flatten targets\n","\n","        # Compute loss\n","        loss = criterion(outputs, targets)\n","\n","        # Backpropagation\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","        epoch_loss += loss.item()\n","\n","    print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {epoch_loss / len(train_dataloader)}\")\n","\n","# # Step 4: Evaluation\n","# def evaluate_model(model, dataloader):\n","#     model.eval()\n","#     total_loss = 0\n","#     with torch.no_grad():\n","#         for inputs, targets in dataloader:\n","#             inputs = inputs.to(device)\n","#             targets = targets.to(device)\n","\n","#             outputs = model(inputs)\n","#             outputs = outputs.view(-1, output_dim)\n","#             targets = targets.view(-1)\n","\n","#             loss = criterion(outputs, targets)\n","#             total_loss += loss.item()\n","#     return total_loss / len(dataloader)\n","\n","# # Test the model (using train data as test data for simplicity here)\n","# test_loss = evaluate_model(model, train_dataloader)\n","# print(f\"Test Loss: {test_loss}\")\n"]},{"cell_type":"code","execution_count":38,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":422,"status":"ok","timestamp":1734189180847,"user":{"displayName":"Vinay Moharir","userId":"16254761846626058829"},"user_tz":-330},"id":"YlBpbYNKfmKY","outputId":"9223de92-5411-4410-e52d-02174e8db261"},"outputs":[{"output_type":"stream","name":"stdout","text":["Sample prediction: [52240  4653 17732 52241 52242  3769   555 52243  1349  1697  7935     0\n","     0     0     0     0     0     0     0     0     0     0     0     0\n","     0     0     0     0     0     0     0     0     0     0     0     0\n","     0     0     0     0     0     0     0     0     0     0     0     0\n","     0     0]\n"]}],"source":["input_dim = train_df[\"English\"].apply(lambda x: max(x)).max() + 1  # Vocabulary size of English\n","model.eval()\n","with torch.no_grad():\n","    for inputs, targets in train_dataloader:\n","        inputs = inputs.to(device) # Move inputs to the correct device\n","        # Removed one-hot encoding - the embedding layer expects integer indices\n","        outputs = model(inputs)\n","        print(\"Sample prediction:\", torch.argmax(outputs, dim=2)[0].cpu().numpy())\n","        break"]},{"cell_type":"code","execution_count":39,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":34939,"status":"ok","timestamp":1734189219214,"user":{"displayName":"Vinay Moharir","userId":"16254761846626058829"},"user_tz":-330},"id":"nKncI7avzaV6","outputId":"d5ae886b-22dd-4138-973a-704970d3f40c"},"outputs":[{"output_type":"stream","name":"stdout","text":["Average BLEU Score: 0.6180\n"]}],"source":["from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n","\n","# Define a function to compute BLEU score\n","def compute_bleu_score(reference, hypothesis):\n","    \"\"\"\n","    Compute BLEU score for a single pair of reference and hypothesis sequences.\n","    :param reference: List of integers (ground truth sequence)\n","    :param hypothesis: List of integers (predicted sequence)\n","    :return: BLEU score\n","    \"\"\"\n","    smoothing = SmoothingFunction().method1\n","    return sentence_bleu([reference], hypothesis, smoothing_function=smoothing)\n","\n","# Evaluate BLEU score on a batch\n","model.eval()\n","total_bleu = 0\n","count = 0\n","\n","with torch.no_grad():\n","    for inputs, targets in train_dataloader:\n","        # inputs_one_hot = torch.nn.functional.one_hot(inputs, num_classes=input_dim).float().to(device)\n","        inputs = inputs.to(device) # Move the inputs to the appropriate device\n","        targets = targets.to(device)\n","\n","        # Forward pass\n","        outputs = model(inputs) # Pass the original inputs (integer indices)\n","\n","        # Get the predicted sequences\n","        predictions = torch.argmax(outputs, dim=2)  # Shape: [batch_size, seq_length]\n","\n","        # Compute BLEU for each sequence in the batch\n","        for i in range(predictions.size(0)):\n","            reference = targets[i].cpu().tolist()\n","            hypothesis = predictions[i].cpu().tolist()\n","\n","            # Remove padding tokens (0s)\n","            reference = [token for token in reference if token != 0]\n","            hypothesis = [token for token in hypothesis if token != 0]\n","\n","            bleu_score = compute_bleu_score(reference, hypothesis)\n","            total_bleu += bleu_score\n","            count += 1\n","\n","# Compute the average BLEU score for the dataset\n","average_bleu = total_bleu / count\n","print(f\"Average BLEU Score: {average_bleu:.4f}\")"]},{"cell_type":"code","source":["# Saving the Bidirectional LSTM model\n","torch.save(model.state_dict(), \"/content/drive/MyDrive/Colab Notebooks/Capstone_VinayM/Bidirectional_LSTM_model.pth\")"],"metadata":{"id":"hgQHGKy73Bzz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["loaded_model = BidirectionalRNN(input_dim, embed_dim, hidden_dim, output_dim)\n","loaded_model.load_state_dict(torch.load(\"/content/drive/MyDrive/Colab Notebooks/Capstone_VinayM/Bidirectional_LSTM_model.pth\"))\n","loaded_model.to(device)"],"metadata":{"id":"-cr_6kor3LAd"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}