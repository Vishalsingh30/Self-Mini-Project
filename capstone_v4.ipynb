{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyN+QvKgDCZ4uycw8c6fqK/z"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"s7-oG3VwUP5t","executionInfo":{"status":"ok","timestamp":1732434365466,"user_tz":-330,"elapsed":3888,"user":{"displayName":"Vishal Singh","userId":"10752893921649395524"}},"outputId":"ee760d6c-3a37-4c07-a3dd-35463b141cce"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["\n","Path = '/content/drive/MyDrive/MLT+Dataset/Dataset/'"],"metadata":{"id":"yr3Nn3PAUQgo","executionInfo":{"status":"ok","timestamp":1732434365466,"user_tz":-330,"elapsed":2,"user":{"displayName":"Vishal Singh","userId":"10752893921649395524"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","import re\n","import tensorflow as tf\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","import numpy as np\n","import os\n","from nltk.stem import PorterStemmer"],"metadata":{"id":"ZE-LKmj3vBtj","executionInfo":{"status":"ok","timestamp":1732434376585,"user_tz":-330,"elapsed":11121,"user":{"displayName":"Vishal Singh","userId":"10752893921649395524"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["\n","# Load each English and German file\n","german_files = [\"commoncrawl_de_en.txt\", \"europarl-v7_de_en.txt\", \"news-commentary-v9_de_en.txt\"]\n","english_files = [\"commoncrawl_en_de.txt\", \"europarl-v7_en_de.txt\", \"news-commentary-v9_en_de.txt\"]"],"metadata":{"id":"3bYX41cbUgpH","executionInfo":{"status":"ok","timestamp":1732435648203,"user_tz":-330,"elapsed":414,"user":{"displayName":"Vishal Singh","userId":"10752893921649395524"}}},"execution_count":15,"outputs":[]},{"cell_type":"code","source":["\n","# Read English and German text files\n","with open(Path + 'commoncrawl_en_de.txt', 'r', encoding='utf-8') as eng_file:\n","    english_sentences = eng_file.readlines()\n","\n","with open(Path + 'commoncrawl_de_en.txt', 'r', encoding='utf-8') as ger_file:\n","    german_sentences = ger_file.readlines()\n","\n","# Strip any unnecessary whitespace (e.g., newline characters)\n","commoncrawl_english_sentences = [sentence.strip() for sentence in english_sentences]\n","commoncrawl_german_sentences = [sentence.strip() for sentence in german_sentences]"],"metadata":{"id":"GNaVM1_-YVCC","executionInfo":{"status":"ok","timestamp":1732435653948,"user_tz":-330,"elapsed":5127,"user":{"displayName":"Vishal Singh","userId":"10752893921649395524"}}},"execution_count":16,"outputs":[]},{"cell_type":"code","source":["if len(commoncrawl_english_sentences) != len(commoncrawl_german_sentences):\n","    raise ValueError(\"The number of sentences in the English and German files do not match.\")"],"metadata":{"id":"X3tRGwWGb87X","executionInfo":{"status":"ok","timestamp":1732435653950,"user_tz":-330,"elapsed":5,"user":{"displayName":"Vishal Singh","userId":"10752893921649395524"}}},"execution_count":17,"outputs":[]},{"cell_type":"code","source":["commoncrawl_en_de_df = pd.DataFrame({\n","    'English': commoncrawl_english_sentences,\n","    'German': commoncrawl_german_sentences\n","})"],"metadata":{"id":"saFKQuzfcCAq","executionInfo":{"status":"ok","timestamp":1732435654654,"user_tz":-330,"elapsed":708,"user":{"displayName":"Vishal Singh","userId":"10752893921649395524"}}},"execution_count":18,"outputs":[]},{"cell_type":"code","source":["commoncrawl_en_de_df.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WBJcQmmicFKt","executionInfo":{"status":"ok","timestamp":1732435654654,"user_tz":-330,"elapsed":4,"user":{"displayName":"Vishal Singh","userId":"10752893921649395524"}},"outputId":"98f1b31e-d5cc-4d95-e5dd-4d230762f0fc"},"execution_count":19,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(2399123, 2)"]},"metadata":{},"execution_count":19}]},{"cell_type":"code","source":["# Read English and German text files\n","with open(Path + 'europarl-v7_en_de.txt', 'r', encoding='utf-8') as eng_file:\n","    english_sentences = eng_file.readlines()\n","\n","with open(Path + 'europarl-v7_de_en.txt', 'r', encoding='utf-8') as ger_file:\n","    german_sentences = ger_file.readlines()\n","\n","# Strip any unnecessary whitespace (e.g., newline characters)\n","europarl_english_sentences = [sentence.strip() for sentence in english_sentences]\n","europarl_german_sentences = [sentence.strip() for sentence in german_sentences]"],"metadata":{"id":"MOXbQ5bYcGj4","executionInfo":{"status":"ok","timestamp":1732435659107,"user_tz":-330,"elapsed":4457,"user":{"displayName":"Vishal Singh","userId":"10752893921649395524"}}},"execution_count":20,"outputs":[]},{"cell_type":"code","source":["if len(europarl_english_sentences) != len(europarl_german_sentences):\n","    raise ValueError(\"The number of sentences in the English and German files do not match.\")"],"metadata":{"id":"TzVxUEyhdZqI","executionInfo":{"status":"ok","timestamp":1732435659108,"user_tz":-330,"elapsed":2,"user":{"displayName":"Vishal Singh","userId":"10752893921649395524"}}},"execution_count":21,"outputs":[]},{"cell_type":"code","source":["\n","europarl_en_de_df = pd.DataFrame({\n","    'English': europarl_english_sentences,\n","    'German': europarl_english_sentences\n","})"],"metadata":{"id":"7VIbwnMldiFw","executionInfo":{"status":"ok","timestamp":1732435659817,"user_tz":-330,"elapsed":711,"user":{"displayName":"Vishal Singh","userId":"10752893921649395524"}}},"execution_count":22,"outputs":[]},{"cell_type":"code","source":["europarl_en_de_df.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pP6xZnhsbbKl","executionInfo":{"status":"ok","timestamp":1732435659817,"user_tz":-330,"elapsed":6,"user":{"displayName":"Vishal Singh","userId":"10752893921649395524"}},"outputId":"8e0f747f-23bf-45b6-86e3-80bc8838b42b"},"execution_count":23,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(1920209, 2)"]},"metadata":{},"execution_count":23}]},{"cell_type":"code","source":["# commentary_english_sentences = [sentence.strip() for sentence in english_sentences if sentence.strip()]\n","# commentary_german_sentences = [sentence.strip() for sentence in german_sentences if sentence.strip()]"],"metadata":{"id":"acS7DzCldoy1","executionInfo":{"status":"ok","timestamp":1732432250919,"user_tz":-330,"elapsed":4,"user":{"displayName":"Vishal Singh","userId":"10752893921649395524"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"cGjbf79rguWX","executionInfo":{"status":"ok","timestamp":1732432250919,"user_tz":-330,"elapsed":4,"user":{"displayName":"Vishal Singh","userId":"10752893921649395524"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","source":["# Read English and German text files\n","# cleaning data for news comm in german\n","def is_special_characters(line):\n","    return re.match(r'^[^a-zA-Z0-9\\s]+$', line) is not None\n","\n","commentary_english_sentences = []\n","commentary_german_sentences = []\n","\n","with open(Path + 'news-commentary-v9_en_de.txt', 'r', encoding='utf-8') as file:\n","  for line in file:\n","        cleaned_line = line.strip()\n","\n","        if cleaned_line and not cleaned_line.isdigit() and not is_special_characters(cleaned_line):\n","            # Replace multiple spaces with a single space\n","            cleaned_line = re.sub(r'\\s+', ' ', cleaned_line)\n","            commentary_english_sentences.append(cleaned_line)\n","\n","with open(Path + 'news-commentary-v9_en_de.txt', 'r', encoding='utf-8') as file:\n","  for line in file:\n","        cleaned_line = line.strip()\n","\n","        if cleaned_line and not cleaned_line.isdigit() and not is_special_characters(cleaned_line):\n","            # Replace multiple spaces with a single space\n","            cleaned_line = re.sub(r'\\s+', ' ', cleaned_line)\n","            commentary_german_sentences.append(cleaned_line)"],"metadata":{"id":"-_oDY4EddqnI","executionInfo":{"status":"ok","timestamp":1732435666098,"user_tz":-330,"elapsed":6285,"user":{"displayName":"Vishal Singh","userId":"10752893921649395524"}}},"execution_count":24,"outputs":[]},{"cell_type":"code","source":["if len(commentary_english_sentences) != len(commentary_german_sentences):\n","    raise ValueError(\"The number of sentences in the English and German files do not match.\")"],"metadata":{"id":"_Kgdo_5Pc9sd","executionInfo":{"status":"ok","timestamp":1732435666099,"user_tz":-330,"elapsed":9,"user":{"displayName":"Vishal Singh","userId":"10752893921649395524"}}},"execution_count":25,"outputs":[]},{"cell_type":"code","source":["commentary_en_de_df = pd.DataFrame({\n","    'English': commentary_english_sentences,\n","    'German': commentary_german_sentences\n","})"],"metadata":{"id":"JtNSJ7DddDxO","executionInfo":{"status":"ok","timestamp":1732435666099,"user_tz":-330,"elapsed":8,"user":{"displayName":"Vishal Singh","userId":"10752893921649395524"}}},"execution_count":26,"outputs":[]},{"cell_type":"code","source":["commentary_en_de_df.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9t_PaaeldWdu","executionInfo":{"status":"ok","timestamp":1732435666099,"user_tz":-330,"elapsed":8,"user":{"displayName":"Vishal Singh","userId":"10752893921649395524"}},"outputId":"2b367252-b9ae-4d4e-edc5-d7be187fdbdd"},"execution_count":27,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(201553, 2)"]},"metadata":{},"execution_count":27}]},{"cell_type":"code","source":["final_df = pd.concat([commoncrawl_en_de_df, europarl_en_de_df, commentary_en_de_df], axis=0, ignore_index=True)"],"metadata":{"id":"I-L0K10bdc-m","executionInfo":{"status":"ok","timestamp":1732436636606,"user_tz":-330,"elapsed":402,"user":{"displayName":"Vishal Singh","userId":"10752893921649395524"}}},"execution_count":64,"outputs":[]},{"cell_type":"code","source":["final_df.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lLQo-HlpdxIx","executionInfo":{"status":"ok","timestamp":1732436597948,"user_tz":-330,"elapsed":5,"user":{"displayName":"Vishal Singh","userId":"10752893921649395524"}},"outputId":"6c622656-cdc2-426e-afdc-0bca6458b156"},"execution_count":59,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(4520885, 2)"]},"metadata":{},"execution_count":59}]},{"cell_type":"code","source":["500000/4520885"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-5knoOYENndw","executionInfo":{"status":"ok","timestamp":1732436141282,"user_tz":-330,"elapsed":452,"user":{"displayName":"Vishal Singh","userId":"10752893921649395524"}},"outputId":"5e29e0d7-c8d3-4746-c220-63b098c09b0f"},"execution_count":47,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.1105978143659925"]},"metadata":{},"execution_count":47}]},{"cell_type":"code","source":["# Cleaning"],"metadata":{"id":"R29NQFVutJo4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_df = final_df.sample(frac=0.011, random_state=12)\n","hold_out_df = final_df.sample(frac=0.01, random_state=22)"],"metadata":{"id":"7AJy4TSkAj4O","executionInfo":{"status":"ok","timestamp":1732436640060,"user_tz":-330,"elapsed":422,"user":{"displayName":"Vishal Singh","userId":"10752893921649395524"}}},"execution_count":65,"outputs":[]},{"cell_type":"code","source":["final_df.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MsYQXaFEPc1Q","executionInfo":{"status":"ok","timestamp":1732436642288,"user_tz":-330,"elapsed":428,"user":{"displayName":"Vishal Singh","userId":"10752893921649395524"}},"outputId":"d12757ec-61e4-4fa5-e09d-b0d1e12a674b"},"execution_count":66,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(4520885, 2)"]},"metadata":{},"execution_count":66}]},{"cell_type":"code","source":["def clean_text(text):\n","    # Convert text to lowercase\n","    text = text.lower()\n","    # Remove special characters and punctuation (except apostrophes in contractions)\n","    text = re.sub(r\"[^a-zA-ZäöüßÄÖÜéèàùâêîôûç'\\s]\", '', text)\n","    # Remove extra whitespace\n","    text = re.sub(r'\\s+', ' ', text).strip()\n","    return text\n","\n","# Apply cleaning function to both columns\n","train_df['English'] = train_df['English'].apply(clean_text)\n","train_df['German'] = train_df['German'].apply(clean_text)\n","\n","# Display cleaned DataFrame\n","print(train_df)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"89j_R0lRfV4o","executionInfo":{"status":"ok","timestamp":1732436666137,"user_tz":-330,"elapsed":1595,"user":{"displayName":"Vishal Singh","userId":"10752893921649395524"}},"outputId":"3a2b57c6-f956-42f2-e151-c75bd69f72dc"},"execution_count":67,"outputs":[{"output_type":"stream","name":"stdout","text":["                                                   English  \\\n","2067265           the counterpart appeared two years later   \n","152051             public parking is available metres away   \n","1684613  we guarantee the return of amount paid for the...   \n","2757674  that question was put to the citizens of switz...   \n","3663922  we welcome that because we the united states n...   \n","...                                                    ...   \n","2177426  since the beginning of they have been collabor...   \n","3777036  standardised eu regulations have made crossbor...   \n","622707   without covering the dish and contents with th...   \n","883410   use the yellow keys on the left to indicate yo...   \n","425100   there is a common misconception that the smili...   \n","\n","                                                    German  \n","2067265         das gegenstück erscheint zwei jahre später  \n","152051   öffentliche parkmöglichkeiten befinden sich in...  \n","1684613  wir garantieren die zurückzahlung der für die ...  \n","2757674  that question was put to the citizens of switz...  \n","3663922  we welcome that because we the united states n...  \n","...                                                    ...  \n","2177426  im web excellence forum verständigen sich komm...  \n","3777036  standardised eu regulations have made crossbor...  \n","622707   trockenmasse von kondensmilch die nach dieser ...  \n","883410   durch anklicken der gelben schaltfelder in der...  \n","425100   abschließend wies penn darauf hin dass die sch...  \n","\n","[49730 rows x 2 columns]\n"]}]},{"cell_type":"code","source":["def clean_text(text):\n","    # Convert text to lowercase\n","    text = text.lower()\n","    # Remove special characters and punctuation (except apostrophes in contractions)\n","    text = re.sub(r\"[^a-zA-ZäöüßÄÖÜéèàùâêîôûç'\\s]\", '', text)\n","    # Remove extra whitespace\n","    text = re.sub(r'\\s+', ' ', text).strip()\n","    return text\n","\n","# Apply cleaning function to both columns\n","hold_out_df['English'] = hold_out_df['English'].apply(clean_text)\n","hold_out_df['German'] = hold_out_df['German'].apply(clean_text)\n","\n","# Display cleaned DataFrame\n","print(hold_out_df)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"S0GCXg6kP5xH","executionInfo":{"status":"ok","timestamp":1732436672154,"user_tz":-330,"elapsed":1682,"user":{"displayName":"Vishal Singh","userId":"10752893921649395524"}},"outputId":"decac66b-0eb1-49a9-c011-eca29c72d6bf"},"execution_count":68,"outputs":[{"output_type":"stream","name":"stdout","text":["                                                   English  \\\n","2703086  the commission has offered to cooperate in the...   \n","971216   these persons can also subscribe to inquiries ...   \n","1810753  um so wichtiger und schoner ist die realtime p...   \n","4465035  on balance that may even make membership more ...   \n","343285   excellent clean nice neighbourhood near the be...   \n","...                                                    ...   \n","207572   wherefore be faithful stand in the office whic...   \n","1279568        thank you for your interest in our products   \n","2761868           i shall stop the list there mr president   \n","93433    room service meetingbanquet facilities airport...   \n","4404829               so why do they have any rights to it   \n","\n","                                                    German  \n","2703086  the commission has offered to cooperate in the...  \n","971216   sie können anfragen von journalisten abonniere...  \n","1810753  um so wichtiger und schöner ist die realtime p...  \n","4465035  on balance that may even make membership more ...  \n","343285   the concept the size and confort of the villas...  \n","...                                                    ...  \n","207572   darum sei treu steh in dem amt das ich dir bes...  \n","1279568  vielen dank für ihr interesse an unseren produ...  \n","2761868           i shall stop the list there mr president  \n","93433    zimmerservice konferenz und veranstaltungsräum...  \n","4404829               so why do they have any rights to it  \n","\n","[45209 rows x 2 columns]\n"]}]},{"cell_type":"code","source":["train_df = train_df.reset_index()\n","hold_out_df = hold_out_df.reset_index()"],"metadata":{"id":"KM35qozGm3HG","executionInfo":{"status":"ok","timestamp":1732436679874,"user_tz":-330,"elapsed":410,"user":{"displayName":"Vishal Singh","userId":"10752893921649395524"}}},"execution_count":69,"outputs":[]},{"cell_type":"code","source":["# Preprocess"],"metadata":{"id":"dQZ-XGZotPMK","executionInfo":{"status":"ok","timestamp":1732435695839,"user_tz":-330,"elapsed":989,"user":{"displayName":"Vishal Singh","userId":"10752893921649395524"}}},"execution_count":34,"outputs":[]},{"cell_type":"code","source":["train_df.head()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"id":"JFZHpxCOAzXu","executionInfo":{"status":"ok","timestamp":1732436685083,"user_tz":-330,"elapsed":442,"user":{"displayName":"Vishal Singh","userId":"10752893921649395524"}},"outputId":"4062d6bf-987f-4a91-b40d-02c4998fbb5b"},"execution_count":70,"outputs":[{"output_type":"execute_result","data":{"text/plain":["     index                                            English  \\\n","0  2067265           the counterpart appeared two years later   \n","1   152051            public parking is available metres away   \n","2  1684613  we guarantee the return of amount paid for the...   \n","3  2757674  that question was put to the citizens of switz...   \n","4  3663922  we welcome that because we the united states n...   \n","\n","                                              German  \n","0         das gegenstück erscheint zwei jahre später  \n","1  öffentliche parkmöglichkeiten befinden sich in...  \n","2  wir garantieren die zurückzahlung der für die ...  \n","3  that question was put to the citizens of switz...  \n","4  we welcome that because we the united states n...  "],"text/html":["\n","  <div id=\"df-7c6727d8-1d2b-4e3c-b647-785e2a1b3e06\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>index</th>\n","      <th>English</th>\n","      <th>German</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>2067265</td>\n","      <td>the counterpart appeared two years later</td>\n","      <td>das gegenstück erscheint zwei jahre später</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>152051</td>\n","      <td>public parking is available metres away</td>\n","      <td>öffentliche parkmöglichkeiten befinden sich in...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>1684613</td>\n","      <td>we guarantee the return of amount paid for the...</td>\n","      <td>wir garantieren die zurückzahlung der für die ...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>2757674</td>\n","      <td>that question was put to the citizens of switz...</td>\n","      <td>that question was put to the citizens of switz...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>3663922</td>\n","      <td>we welcome that because we the united states n...</td>\n","      <td>we welcome that because we the united states n...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-7c6727d8-1d2b-4e3c-b647-785e2a1b3e06')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-7c6727d8-1d2b-4e3c-b647-785e2a1b3e06 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-7c6727d8-1d2b-4e3c-b647-785e2a1b3e06');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-8cc31b82-9541-49fd-87ca-b77b516b0f07\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-8cc31b82-9541-49fd-87ca-b77b516b0f07')\"\n","            title=\"Suggest charts\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-8cc31b82-9541-49fd-87ca-b77b516b0f07 button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","variable_name":"train_df","summary":"{\n  \"name\": \"train_df\",\n  \"rows\": 49730,\n  \"fields\": [\n    {\n      \"column\": \"index\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1310640,\n        \"min\": 76,\n        \"max\": 4520868,\n        \"num_unique_values\": 49730,\n        \"samples\": [\n          1390687,\n          1418733,\n          2933121\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"English\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 49244,\n        \"samples\": [\n          \"special quality at a reasonable price\",\n          \"some of the amendments put forward by our committee were adopted by the committee on industry external trade research and energy while others were not\",\n          \"at this stage i might say that there has been progress\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"German\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 49278,\n        \"samples\": [\n          \"my question to the council is this do you plan to accept this proposal\",\n          \"eine besonders hohe verbundquote zwischen und prozent ist indikator f\\u00fcr die enge beziehung zwischen sparkassen lbbw und anderen verbundunternehmen\",\n          \"the appeal of the abhorrent forces of kneejerk nationalism on the far right may widen\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"}},"metadata":{},"execution_count":70}]},{"cell_type":"code","source":["import nltk\n","nltk.download('punkt_tab')\n","\n","from nltk.tokenize import word_tokenize"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"khECX4cx-C0_","executionInfo":{"status":"ok","timestamp":1732436688556,"user_tz":-330,"elapsed":440,"user":{"displayName":"Vishal Singh","userId":"10752893921649395524"}},"outputId":"d03e399f-db9f-4094-d49a-07c8259508b0"},"execution_count":71,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n","[nltk_data]   Package punkt_tab is already up-to-date!\n"]}]},{"cell_type":"code","source":["\n","nltk.download('stopwords')\n","\n","from nltk.corpus import stopwords\n","\n","german_stop_words = stopwords.words('german')\n","english_stop_words = stopwords.words('english')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LqB3iDNc-T8B","executionInfo":{"status":"ok","timestamp":1732436690084,"user_tz":-330,"elapsed":6,"user":{"displayName":"Vishal Singh","userId":"10752893921649395524"}},"outputId":"8cb6ca16-e19d-4798-a1b3-26d5eb5fa8bd"},"execution_count":72,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"]}]},{"cell_type":"code","source":["chunk_size = 500000  # Adjust this based on available memory\n","max_seq_length = 50  # Adjust based on data analysis\n","output_dir = '/content/data1117/preprocessed_chunks_training/'\n","os.makedirs(output_dir, exist_ok=True)\n","\n","# Initialize tokenizers\n","stemmer = PorterStemmer()\n","english_tokenizer = Tokenizer()\n","german_tokenizer = Tokenizer()\n","\n","# Split the DataFrame into chunks manually\n","num_chunks = len(train_df) // chunk_size + (1 if len(train_df) % chunk_size != 0 else 0)\n","\n","# Step 1: Fit Tokenizers Across Chunks\n","for i in range(num_chunks):\n","    print(\"num_chunk----->\", i)\n","    chunk = train_df.iloc[i * chunk_size:(i + 1) * chunk_size]\n","    chunk['german_tokens'] = chunk['German'].apply(word_tokenize)\n","    chunk['english_tokens'] = chunk['English'].apply(word_tokenize)\n","    chunk['german_tokens'] = chunk['german_tokens'].apply(lambda tokens: [token for token in tokens if token not in german_stop_words])\n","    chunk['english_tokens'] = chunk['english_tokens'].apply(lambda tokens: [token for token in tokens if token not in english_stop_words])\n","\n","    chunk['german_tokens'] = chunk['german_tokens'].apply(lambda tokens: [stemmer.stem(token) for token in tokens])\n","    chunk['english_tokens'] = chunk['english_tokens'].apply(lambda tokens: [stemmer.stem(token) for token in tokens])\n","    chunk['German'] = chunk['german_tokens'].apply(lambda tokens: ' '.join(tokens))\n","    chunk['English'] = chunk['english_tokens'].apply(lambda tokens: ' '.join(tokens))\n","\n","    # Update tokenizers\n","    english_tokenizer.fit_on_texts(chunk['English'])\n","    german_tokenizer.fit_on_texts(chunk['German'])\n","\n","    # Convert to sequences\n","    english_sequences = english_tokenizer.texts_to_sequences(chunk['English'])\n","    german_sequences = german_tokenizer.texts_to_sequences(chunk['German'])\n","\n","    # Pad sequences\n","    english_padded = pad_sequences(english_sequences, maxlen=max_seq_length, padding='post')\n","    german_padded = pad_sequences(german_sequences, maxlen=max_seq_length, padding='post')\n","\n","    # Save each chunk\n","    np.save(os.path.join(output_dir, f'english_chunk_{i}.npy'), english_padded)\n","    np.save(os.path.join(output_dir, f'german_chunk_{i}.npy'), german_padded)\n","\n","print(\"Data processing in chunks completed. Tokenized sequences are saved to disk.\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"McCsEHNwM83Q","executionInfo":{"status":"ok","timestamp":1732436756152,"user_tz":-330,"elapsed":53087,"user":{"displayName":"Vishal Singh","userId":"10752893921649395524"}},"outputId":"7df8cc5c-f943-4ea6-ffcd-113647d583fd"},"execution_count":73,"outputs":[{"output_type":"stream","name":"stdout","text":["num_chunk-----> 0\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-73-be82efb5eff6>:18: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  chunk['german_tokens'] = chunk['German'].apply(word_tokenize)\n","<ipython-input-73-be82efb5eff6>:19: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  chunk['english_tokens'] = chunk['English'].apply(word_tokenize)\n","<ipython-input-73-be82efb5eff6>:20: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  chunk['german_tokens'] = chunk['german_tokens'].apply(lambda tokens: [token for token in tokens if token not in german_stop_words])\n","<ipython-input-73-be82efb5eff6>:21: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  chunk['english_tokens'] = chunk['english_tokens'].apply(lambda tokens: [token for token in tokens if token not in english_stop_words])\n","<ipython-input-73-be82efb5eff6>:23: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  chunk['german_tokens'] = chunk['german_tokens'].apply(lambda tokens: [stemmer.stem(token) for token in tokens])\n","<ipython-input-73-be82efb5eff6>:24: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  chunk['english_tokens'] = chunk['english_tokens'].apply(lambda tokens: [stemmer.stem(token) for token in tokens])\n","<ipython-input-73-be82efb5eff6>:25: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  chunk['German'] = chunk['german_tokens'].apply(lambda tokens: ' '.join(tokens))\n","<ipython-input-73-be82efb5eff6>:26: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  chunk['English'] = chunk['english_tokens'].apply(lambda tokens: ' '.join(tokens))\n"]},{"output_type":"stream","name":"stdout","text":["Data processing in chunks completed. Tokenized sequences are saved to disk.\n"]}]},{"cell_type":"code","source":["chunk_size = 500000  # Adjust this based on available memory\n","max_seq_length = 50  # Adjust based on data analysis\n","output_dir = '/content/data1117/preprocessed_chunks_holdout/'\n","os.makedirs(output_dir, exist_ok=True)\n","\n","# Initialize tokenizers\n","stemmer = PorterStemmer()\n","english_tokenizer = Tokenizer()\n","german_tokenizer = Tokenizer()\n","\n","# Split the DataFrame into chunks manually\n","num_chunks = len(hold_out_df) // chunk_size + (1 if len(hold_out_df) % chunk_size != 0 else 0)\n","\n","# Step 1: Fit Tokenizers Across Chunks\n","for i in range(num_chunks):\n","    print(\"num_chunk----->\", i)\n","    chunk = hold_out_df.iloc[i * chunk_size:(i + 1) * chunk_size]\n","    chunk['german_tokens'] = chunk['German'].apply(word_tokenize)\n","    chunk['english_tokens'] = chunk['English'].apply(word_tokenize)\n","    chunk['german_tokens'] = chunk['german_tokens'].apply(lambda tokens: [token for token in tokens if token not in german_stop_words])\n","    chunk['english_tokens'] = chunk['english_tokens'].apply(lambda tokens: [token for token in tokens if token not in english_stop_words])\n","\n","    chunk['german_tokens'] = chunk['german_tokens'].apply(lambda tokens: [stemmer.stem(token) for token in tokens])\n","    chunk['english_tokens'] = chunk['english_tokens'].apply(lambda tokens: [stemmer.stem(token) for token in tokens])\n","    chunk['German'] = chunk['german_tokens'].apply(lambda tokens: ' '.join(tokens))\n","    chunk['English'] = chunk['english_tokens'].apply(lambda tokens: ' '.join(tokens))\n","\n","    # Update tokenizers\n","    english_tokenizer.fit_on_texts(chunk['English'])\n","    german_tokenizer.fit_on_texts(chunk['German'])\n","\n","    # Convert to sequences\n","    english_sequences = english_tokenizer.texts_to_sequences(chunk['English'])\n","    german_sequences = german_tokenizer.texts_to_sequences(chunk['German'])\n","\n","    # Pad sequences\n","    english_padded = pad_sequences(english_sequences, maxlen=max_seq_length, padding='post')\n","    german_padded = pad_sequences(german_sequences, maxlen=max_seq_length, padding='post')\n","\n","    # Save each chunk\n","    np.save(os.path.join(output_dir, f'english_chunk_{i}.npy'), english_padded)\n","    np.save(os.path.join(output_dir, f'german_chunk_{i}.npy'), german_padded)\n","\n","print(\"Data processing in chunks completed. Tokenized sequences are saved to disk.\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XkqciiefQYwf","executionInfo":{"status":"ok","timestamp":1732436803490,"user_tz":-330,"elapsed":47343,"user":{"displayName":"Vishal Singh","userId":"10752893921649395524"}},"outputId":"6e6891cc-d21d-487b-cd2c-d08c9c975bec"},"execution_count":74,"outputs":[{"output_type":"stream","name":"stdout","text":["num_chunk-----> 0\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-74-ff2f5c8d65ae>:18: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  chunk['german_tokens'] = chunk['German'].apply(word_tokenize)\n","<ipython-input-74-ff2f5c8d65ae>:19: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  chunk['english_tokens'] = chunk['English'].apply(word_tokenize)\n","<ipython-input-74-ff2f5c8d65ae>:20: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  chunk['german_tokens'] = chunk['german_tokens'].apply(lambda tokens: [token for token in tokens if token not in german_stop_words])\n","<ipython-input-74-ff2f5c8d65ae>:21: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  chunk['english_tokens'] = chunk['english_tokens'].apply(lambda tokens: [token for token in tokens if token not in english_stop_words])\n","<ipython-input-74-ff2f5c8d65ae>:23: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  chunk['german_tokens'] = chunk['german_tokens'].apply(lambda tokens: [stemmer.stem(token) for token in tokens])\n","<ipython-input-74-ff2f5c8d65ae>:24: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  chunk['english_tokens'] = chunk['english_tokens'].apply(lambda tokens: [stemmer.stem(token) for token in tokens])\n","<ipython-input-74-ff2f5c8d65ae>:25: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  chunk['German'] = chunk['german_tokens'].apply(lambda tokens: ' '.join(tokens))\n","<ipython-input-74-ff2f5c8d65ae>:26: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  chunk['English'] = chunk['english_tokens'].apply(lambda tokens: ' '.join(tokens))\n"]},{"output_type":"stream","name":"stdout","text":["Data processing in chunks completed. Tokenized sequences are saved to disk.\n"]}]},{"cell_type":"code","source":["# Initialize lists to store all data\n","english_data = []\n","german_data = []\n","output_dir = '/content/data1117/preprocessed_chunks_training/'\n","# Iterate through all saved chunks\n","num_chunks = len([name for name in os.listdir(output_dir) if name.startswith('english_chunk_')])\n","\n","for i in range(num_chunks):\n","    # Load the English and German chunks\n","    english_chunk = np.load(os.path.join(output_dir, f'english_chunk_{i}.npy'))\n","    german_chunk = np.load(os.path.join(output_dir, f'german_chunk_{i}.npy'))\n","\n","    # Append to the list\n","    english_data.extend(english_chunk)\n","    german_data.extend(german_chunk)\n","\n","# Convert lists to DataFrame\n","preprocessed_training_data = pd.DataFrame({\n","    'English': english_data,\n","    'German': german_data\n","})\n","\n","print(\"Combined DataFrame created successfully.\")\n","print(preprocessed_training_data.head())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2inGit1MX16j","executionInfo":{"status":"ok","timestamp":1732436847993,"user_tz":-330,"elapsed":406,"user":{"displayName":"Vishal Singh","userId":"10752893921649395524"}},"outputId":"fca800c5-9f5d-498a-d98d-1dd9fe469c2f"},"execution_count":76,"outputs":[{"output_type":"stream","name":"stdout","text":["Combined DataFrame created successfully.\n","                                             English  \\\n","0  [3983, 530, 56, 19, 676, 0, 0, 0, 0, 0, 0, 0, ...   \n","1  [76, 336, 123, 1431, 459, 0, 0, 0, 0, 0, 0, 0,...   \n","2  [387, 555, 529, 1249, 32, 0, 0, 0, 0, 0, 0, 0,...   \n","3  [84, 178, 242, 1480, 0, 0, 0, 0, 0, 0, 0, 0, 0...   \n","4  [282, 177, 8, 14, 790, 2443, 1504, 93, 22, 138...   \n","\n","                                              German  \n","0  [19035, 2690, 256, 128, 1063, 0, 0, 0, 0, 0, 0...  \n","1  [1530, 12330, 928, 316, 3189, 0, 0, 0, 0, 0, 0...  \n","2  [3190, 19036, 242, 28666, 7379, 0, 0, 0, 0, 0,...  \n","3  [6, 124, 205, 3, 1, 208, 2, 5539, 0, 0, 0, 0, ...  \n","4  [10, 406, 6, 88, 10, 1, 276, 33, 51, 929, 3462...  \n"]}]},{"cell_type":"code","source":["# Initialize lists to store all data\n","english_data = []\n","german_data = []\n","output_dir = '/content/data1117/preprocessed_chunks_holdout/'\n","# Iterate through all saved chunks\n","num_chunks = len([name for name in os.listdir(output_dir) if name.startswith('english_chunk_')])\n","\n","for i in range(num_chunks):\n","    # Load the English and German chunks\n","    english_chunk = np.load(os.path.join(output_dir, f'english_chunk_{i}.npy'))\n","    german_chunk = np.load(os.path.join(output_dir, f'german_chunk_{i}.npy'))\n","\n","    # Append to the list\n","    english_data.extend(english_chunk)\n","    german_data.extend(german_chunk)\n","\n","# Convert lists to DataFrame\n","preprocessed_holdout_data = pd.DataFrame({\n","    'English': english_data,\n","    'German': german_data\n","})\n","\n","print(\"Combined DataFrame created successfully.\")\n","print(preprocessed_holdout_data.head())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Un7ZxfoFQP6Q","executionInfo":{"status":"ok","timestamp":1732436867292,"user_tz":-330,"elapsed":414,"user":{"displayName":"Vishal Singh","userId":"10752893921649395524"}},"outputId":"314067c5-75aa-49c0-d3f8-d5e2bf68cc87"},"execution_count":77,"outputs":[{"output_type":"stream","name":"stdout","text":["Combined DataFrame created successfully.\n","                                             English  \\\n","0  [10, 40, 199, 415, 42, 508, 0, 0, 0, 0, 0, 0, ...   \n","1  [111, 2, 2432, 2111, 2557, 798, 1051, 1512, 20...   \n","2  [3630, 11267, 1035, 16200, 3398, 839, 3631, 16...   \n","3  [547, 56, 57, 22, 1302, 1292, 9013, 2745, 1473...   \n","4  [347, 738, 766, 2279, 635, 428, 311, 1941, 140...   \n","\n","                                              German  \n","0  [1, 30, 23, 563, 3, 191, 1, 605, 2, 12, 88, 68...  \n","1  [5053, 7483, 6936, 9035, 10181, 1278, 0, 0, 0,...  \n","2  [664, 5054, 10182, 26879, 17785, 5659, 3905, 1...  \n","3  [13, 511, 6, 190, 139, 72, 1516, 48, 1615, 8, ...  \n","4  [1, 1429, 1, 2377, 4, 5660, 2, 1, 1343, 1, 422...  \n"]}]},{"cell_type":"code","source":["preprocessed_training_data.to_parquet('/content/drive/MyDrive/capstone_preprocess_training.parquet')\n","preprocessed_holdout_data.to_parquet('/content/drive/MyDrive/capstone_preprocess_holdout.parquet')"],"metadata":{"id":"35gKhDymrfDT","executionInfo":{"status":"ok","timestamp":1732436932508,"user_tz":-330,"elapsed":435,"user":{"displayName":"Vishal Singh","userId":"10752893921649395524"}}},"execution_count":78,"outputs":[]},{"cell_type":"code","source":["train_df = pd.read_parquet('/content/drive/MyDrive/capstone_preprocess_training.parquet')\n","hold_out_df = pd.read_parquet('/content/drive/MyDrive/capstone_preprocess_holdout.parquet')"],"metadata":{"id":"hBwlZ0ZwzH-l","executionInfo":{"status":"ok","timestamp":1732436957709,"user_tz":-330,"elapsed":416,"user":{"displayName":"Vishal Singh","userId":"10752893921649395524"}}},"execution_count":79,"outputs":[]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import DataLoader, Dataset\n","import pandas as pd\n","import numpy as np\n","\n","# Step 1: Custom Dataset Class\n","class SequenceDataset(Dataset):\n","    def __init__(self, df, input_col, target_col, input_max_len=None, target_max_len=None):\n","        self.input_sequences = self.pad_sequences(df[input_col].tolist(), max_length=input_max_len)\n","        self.target_sequences = self.pad_sequences(df[target_col].tolist(), max_length=target_max_len)\n","\n","    def pad_sequences(self, sequences, max_length=None):\n","        valid_sequences = [seq for seq in sequences if len(seq) > 0]\n","        if max_length is None:\n","            max_length = max(len(seq) for seq in valid_sequences)\n","        return np.array(\n","            valid_sequences, dtype=np.int64\n","        )\n","\n","    def __len__(self):\n","        return len(self.input_sequences)\n","\n","    def __getitem__(self, idx):\n","        return (\n","            torch.tensor(self.input_sequences[idx], dtype=torch.long),\n","            torch.tensor(self.target_sequences[idx], dtype=torch.long),\n","        )\n","\n","# Load and preprocess data\n","# Assume df is a Pandas DataFrame with \"English\" and \"German\" columns\n","input_max_len = 50  # Set maximum sequence lengths\n","target_max_len = 50\n","train_dataset = SequenceDataset(train_df, input_col=\"English\", target_col=\"German\", input_max_len=input_max_len, target_max_len=target_max_len)\n","hold_out_dataset = SequenceDataset(hold_out_df, input_col=\"English\", target_col=\"German\", input_max_len=input_max_len, target_max_len=target_max_len)\n","\n","# DataLoader\n","batch_size = 32\n","train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","holdout_dataloader = DataLoader(hold_out_dataset, batch_size=batch_size, shuffle=True)\n","\n","# Step 2: Define the RNN Model\n","class SimpleRNN(nn.Module):\n","    def __init__(self, input_dim, output_dim, hidden_dim):\n","        super(SimpleRNN, self).__init__()\n","        self.hidden_dim = hidden_dim\n","        self.rnn = nn.RNN(input_dim, hidden_dim, batch_first=True)\n","        self.fc = nn.Linear(hidden_dim, output_dim)\n","\n","    def forward(self, x):\n","        h_0 = torch.zeros(1, x.size(0), self.hidden_dim).to(x.device)  # Initialize hidden state\n","        output, _ = self.rnn(x, h_0)  # RNN forward pass\n","        output = self.fc(output)  # Fully connected layer\n","        return output\n","\n","# Model parameters\n","input_dim = train_df[\"English\"].apply(lambda x: max(x)).max() + 1  # Vocabulary size of English\n","output_dim = train_df[\"German\"].apply(lambda x: max(x)).max() + 1  # Vocabulary size of German\n","hidden_dim = 128\n","\n","# Initialize model, loss, and optimizer\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(device)\n","model = SimpleRNN(input_dim, output_dim, hidden_dim).to(device)\n","criterion = nn.CrossEntropyLoss()  # CrossEntropyLoss expects raw logits\n","optimizer = optim.Adam(model.parameters(), lr=0.001)\n","\n","# Step 3: Training Loop\n","num_epochs = 10\n","for epoch in range(num_epochs):\n","    model.train()\n","    epoch_loss = 0\n","    for inputs, targets in train_dataloader:\n","        # One-hot encode inputs\n","        inputs_one_hot = torch.nn.functional.one_hot(inputs, num_classes=input_dim).float().to(device)\n","        targets = targets.to(device)\n","\n","        # Forward pass\n","        outputs = model(inputs_one_hot)\n","\n","        # Reshape outputs and targets for loss computation\n","        outputs = outputs.view(-1, output_dim)  # Flatten for CrossEntropyLoss\n","        targets = targets.view(-1)  # Flatten targets\n","\n","        # Compute loss\n","        loss = criterion(outputs, targets)\n","\n","        # Backpropagation\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","        epoch_loss += loss.item()\n","\n","    print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {epoch_loss / len(train_dataloader)}\")\n","\n","# Step 4: Evaluation (optional)\n","model.eval()\n","with torch.no_grad():\n","    for inputs, targets in holdout_dataloader:\n","        inputs_one_hot = torch.nn.functional.one_hot(inputs, num_classes=input_dim).float().to(device)\n","        outputs = model(inputs_one_hot)\n","        print(\"Sample prediction:\", torch.argmax(outputs, dim=2)[0].cpu().numpy())\n","        break\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cVVFkA0bIrSk","outputId":"6c188cfc-aa46-407e-b792-00abfdd41508"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["cuda\n","Epoch 1/10, Loss: 3.189885659693138\n"]}]},{"cell_type":"code","source":["from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n","\n","# Define a function to compute BLEU score\n","def compute_bleu_score(reference, hypothesis):\n","    \"\"\"\n","    Compute BLEU score for a single pair of reference and hypothesis sequences.\n","    :param reference: List of integers (ground truth sequence)\n","    :param hypothesis: List of integers (predicted sequence)\n","    :return: BLEU score\n","    \"\"\"\n","    smoothing = SmoothingFunction().method1\n","    return sentence_bleu([reference], hypothesis, smoothing_function=smoothing)\n","\n","# Evaluate BLEU score on a batch\n","model.eval()\n","total_bleu = 0\n","count = 0\n","\n","with torch.no_grad():\n","    for inputs, targets in holdout_dataloader:\n","        inputs_one_hot = torch.nn.functional.one_hot(inputs, num_classes=input_dim).float().to(device)\n","        targets = targets.to(device)\n","\n","        # Forward pass\n","        outputs = model(inputs_one_hot)\n","\n","        # Get the predicted sequences\n","        predictions = torch.argmax(outputs, dim=2)  # Shape: [batch_size, seq_length]\n","\n","        # Compute BLEU for each sequence in the batch\n","        for i in range(predictions.size(0)):\n","            reference = targets[i].cpu().tolist()\n","            hypothesis = predictions[i].cpu().tolist()\n","\n","            # Remove padding tokens (0s)\n","            reference = [token for token in reference if token != 0]\n","            hypothesis = [token for token in hypothesis if token != 0]\n","\n","            bleu_score = compute_bleu_score(reference, hypothesis)\n","            total_bleu += bleu_score\n","            count += 1\n","\n","# Compute the average BLEU score for the dataset\n","average_bleu = total_bleu / count\n","print(f\"Average BLEU Score: {average_bleu:.4f}\")\n"],"metadata":{"id":"nR80iDhpIrwz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"9ldhm3l_MkY0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"eR8r5jfPIrzv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"wiWCyk2aIr2X"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"y7bBO8m-Ir5u"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"OQCkooZ9Nncm"},"execution_count":null,"outputs":[]}]}